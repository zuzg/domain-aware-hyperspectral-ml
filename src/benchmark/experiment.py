import numpy as np
import wandb
import torch
import torch.nn as nn
from scipy.io import loadmat
from torch.utils.data import DataLoader, Dataset

from src.benchmark.pred_ml import predict_soil_classes
from src.config import ExperimentConfig
from src.consts import RENDERERS_DICT
from src.data.dataset import HyperspectralScene
from src.data.preprocessing import mean_to_bias
from src.eval.eval_loop import Evaluator
from src.models.autoencoder import Autoencoder
from src.models.bias_variance_model import BiasModel, BiasVarianceModel, VarianceModel
from src.models.modeller import Modeller
from src.train.train_loop import pretrain, train


class Experiment:
    def __init__(self, cfg: ExperimentConfig) -> None:
        self.cfg = cfg
        self.ae = self.cfg.variance_renderer == "Autoencoder"
        if self.cfg.wandb:
            self.name, self.tags = self._set_experiment_name_and_tags()
            self._initialize_wandb()

    def _set_experiment_name_and_tags(self) -> tuple[str, list[str]]:
        if self.ae:
            return f"Autoencoder_latent={3*self.cfg.k:.0f}", []
        return f"[B] var={self.cfg.variance_renderer}_bias={self.cfg.bias_renderer}_k={self.cfg.k}", [
            f"Î¼: {self.cfg.mu_type}", "MLP"  # TODO benchmark name
        ]

    def _initialize_wandb(self) -> None:
        wandb.init(
            project="hyperview",
            name=self.name,
            config=vars(self.cfg),
            tags=self.tags,
        )

    def prepare_datasets(self, arr_train: np.ndarray, arr_test: np.ndarray, div: np.ndarray) -> tuple[Dataset]:
        trainset = HyperspectralScene(arr_train, 0, div)
        testset = HyperspectralScene(arr_test, 0, div)
        return trainset, testset
        

    def prepare_dataloaders(
        self, trainset: Dataset, valset: Dataset) -> tuple[DataLoader]:
        return (
            DataLoader(trainset, batch_size=self.cfg.batch_size, shuffle=True, drop_last=False),
            DataLoader(valset, batch_size=self.cfg.batch_size, drop_last=True),
        )

    def _load_max_values(self, arr: np.ndarray) -> np.ndarray:
        arr_max_c = arr.max(axis=(0, 1)).astype(np.float32)
        return arr_max_c

    def _setup_autoencoder(self, div: np.ndarray) -> nn.Module:
        self.num_params = 3
        variance_model =  Autoencoder(self.cfg.channels, self.cfg.k, self.num_params).to(self.cfg.device)
        bias_model = self._prepare_bias_model(div)
        return BiasVarianceModel(bias_model, variance_model)

    def _setup_bias_variance_model(self, div: np.ndarray) -> nn.Module:
        variance_renderer = RENDERERS_DICT[self.cfg.variance_renderer]
        self.num_params = variance_renderer.num_params
        modeller = Modeller(self.cfg.img_size, self.cfg.channels, self.cfg.k, self.num_params).to(self.cfg.device)
        variance_model = VarianceModel(modeller, variance_renderer.model(self.cfg.device, self.cfg.channels, self.cfg.mu_type))
        bias_model = self._prepare_bias_model(div)
        return BiasVarianceModel(bias_model, variance_model)

    def _prepare_bias_model(self, div: np.ndarray) -> nn.Module:
        pavia = loadmat("data/benchmark/PaviaU.mat")["paviaU"]
        pavia_train = pavia[:170, :170]
        pavia_mean_c = pavia_train.mean(axis=(0, 1)).astype(np.float32)
        if self.cfg.bias_renderer == "Mean":
            return mean_to_bias(pavia_mean_c, div, self.cfg.device, self.cfg.img_size, self.cfg.batch_size)

        else:
            bias_renderer = RENDERERS_DICT[self.cfg.bias_renderer]
            bias_renderer_model = bias_renderer.model(self.cfg.device, self.cfg.channels)
            bias_shape = (self.cfg.k, bias_renderer.num_params)
            bias_model = BiasModel(bias_shape, self.cfg.batch_size, self.cfg.img_size, bias_renderer_model, self.cfg.device)

            bias_model = pretrain(bias_model, self.trainloader, self.cfg)
            for param in bias_model.parameters():
                param.requires_grad = False
        return bias_model

    def run(self) -> None:
        torch.manual_seed(42)
        # read data
        pavia = loadmat("data/benchmark/PaviaU.mat")["paviaU"]
        pavia_gt = loadmat("data/benchmark/PaviaU_gt.mat")["paviaU_gt"]
        pavia_train = pavia[:170, :170]
        pavia_test = pavia[:170, 170:]
        pavia_gt_test = pavia_gt[:170, 170:]

        max_values = self._load_max_values(pavia_train)
        trainset, valset = self.prepare_datasets(pavia_train, pavia_test, max_values)
        self.trainloader, self.valloader = self.prepare_dataloaders(trainset, valset)
        model = self._setup_autoencoder(max_values) if self.ae else self._setup_bias_variance_model(max_values)
        model = train(model, self.trainloader, self.valloader, self.cfg)

        modeller = model.variance.encoder if self.ae else model.variance.modeller
        if self.cfg.wandb:
            Evaluator(model, self.cfg, self.ae).evaluate(self.valloader)

        for val_img in self.valloader:
            features = modeller(val_img.to(self.cfg.device))[0]
        print(features.shape)
        features_flat = features.reshape(features.shape[0] * features.shape[1], features.shape[2] * features.shape[3])
        features_flat = features_flat.permute(1, 0)
        print(features_flat.shape)
        pavia_gt_test = np.expand_dims(pavia_gt_test, axis=0)
        print(pavia_gt_test.shape)
        gt_flat = pavia_gt_test.reshape(pavia_gt_test.shape[1] * pavia_gt_test.shape[2], pavia_gt_test.shape[0])
        print(gt_flat.shape)

        predict_soil_classes(features_flat.cpu().detach().numpy(), gt_flat)

        wandb.finish()
